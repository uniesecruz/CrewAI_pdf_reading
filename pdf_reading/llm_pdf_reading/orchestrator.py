"""
Classe principal para orquestra칞칚o do processamento de PDFs
Suporte para LLMs locais gratuitos e APIs comerciais
"""
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging
from .pdf_utils import PDFProcessor
from .config import USE_LOCAL_MODELS, get_model_config

logger = logging.getLogger(__name__)

class PDFReadingOrchestrator:
    """Orquestrador principal para leitura e an치lise de PDFs"""
    
    def __init__(self, llm=None, use_local_models: Optional[bool] = None, custom_config: Optional[Dict] = None):
        self.pdf_processor = PDFProcessor()
        self.use_local_models = use_local_models if use_local_models is not None else USE_LOCAL_MODELS
        self.llm_manager = None
        self.custom_config = custom_config
        
        # Inicializar LLM
        self._initialize_llm(llm)
    
    def _initialize_llm(self, llm=None):
        """Inicializa o gerenciador de LLM"""
        if llm:
            self.llm_manager = llm
            return
        
        if self.use_local_models:
            try:
                # Se h치 configura칞칚o personalizada, usar LocalLLMManager diretamente
                if self.custom_config:
                    from .local_llm import LocalLLMManager
                    self.llm_manager = LocalLLMManager(self.custom_config)
                else:
                    from .local_llm import create_local_llm_manager
                    self.llm_manager = create_local_llm_manager()
                
                if self.llm_manager and self.llm_manager.is_available():
                    logger.info(f"游뱄 LLM local inicializado: {self.llm_manager.current_provider}")
                else:
                    logger.warning("丘멆잺  LLM local n칚o dispon칤vel, usando modo simplificado")
                    self.llm_manager = None
            except Exception as e:
                logger.error(f"Erro ao inicializar LLM local: {e}")
                self.llm_manager = None
        else:
            logger.info("游눯 Modo API comercial selecionado")
            # Aqui seria inicializado cliente para APIs comerciais
            self.llm_manager = None
    
    def process_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """Processa um arquivo PDF completo"""
        try:
            pdf_path = Path(pdf_path)
            
            # Verificar se o arquivo existe
            if not pdf_path.exists():
                raise FileNotFoundError(f"Arquivo n칚o encontrado: {pdf_path}")
            
            # Extrair texto
            logger.info(f"游늯 Extraindo texto de: {pdf_path}")
            text_content = self.pdf_processor.extract_text_pymupdf(pdf_path)
            
            if not text_content.strip():
                raise ValueError("N칚o foi poss칤vel extrair texto do PDF")
            
            # Extrair metadados
            metadata = self.pdf_processor.extract_metadata(pdf_path)
            
            # Dividir em chunks
            chunks = self.pdf_processor.split_into_chunks(text_content)
            
            # Analisar conte칰do com LLM (se dispon칤vel)
            analysis_result = self._analyze_content_with_llm(text_content)
            
            return {
                "file_path": str(pdf_path),
                "metadata": metadata,
                "content": text_content,
                "chunks": chunks,
                "analysis": analysis_result,
                "llm_used": self.llm_manager.current_provider if self.llm_manager else "none",
                "success": True
            }
            
        except Exception as e:
            logger.error(f"Erro ao processar PDF: {e}")
            return {
                "file_path": str(pdf_path) if 'pdf_path' in locals() else "unknown",
                "error": str(e),
                "success": False
            }
    
    def answer_question(self, pdf_content: str, question: str) -> str:
        """Responde uma pergunta sobre o conte칰do do PDF usando LLM"""
        try:
            if self.llm_manager and self.llm_manager.is_available():
                # Criar prompt estruturado
                prompt = f"""
Baseado no seguinte conte칰do de documento, responda a pergunta de forma precisa e concisa.

DOCUMENTO:
{pdf_content[:2000]}...

PERGUNTA: {question}

RESPOSTA:"""
                
                # Gerar resposta com LLM local
                response = self.llm_manager.generate(
                    prompt,
                    max_length=500,
                    temperature=0.3  # Mais determin칤stico para QA
                )
                
                return response.strip()
            else:
                # Fallback simples quando LLM n칚o est치 dispon칤vel
                return self._simple_qa_fallback(pdf_content, question)
                
        except Exception as e:
            logger.error(f"Erro ao responder pergunta: {e}")
            return f"Erro ao processar pergunta: {e}"
    
    def _analyze_content_with_llm(self, content: str) -> Dict[str, Any]:
        """An치lise de conte칰do usando LLM (se dispon칤vel)"""
        base_analysis = self._basic_analysis(content)
        
        if self.llm_manager and self.llm_manager.is_available():
            try:
                # Prompt para an치lise do conte칰do
                prompt = f"""
Analise o seguinte texto e forne칞a:
1. Resumo principal (m치ximo 100 palavras)
2. 3-5 t칩picos principais
3. Tipo de documento (relat칩rio, artigo, manual, etc.)

TEXTO:
{content[:1500]}...

AN츼LISE:"""
                
                llm_analysis = self.llm_manager.generate(
                    prompt,
                    max_length=300,
                    temperature=0.5
                )
                
                base_analysis["llm_summary"] = llm_analysis
                base_analysis["analysis_type"] = "llm_enhanced"
                
            except Exception as e:
                logger.error(f"Erro na an치lise LLM: {e}")
                base_analysis["analysis_type"] = "basic_only"
        
        return base_analysis
    
    def _basic_analysis(self, content: str) -> Dict[str, Any]:
        """An치lise b치sica sem LLM"""
        words = content.split()
        word_count = len(words)
        char_count = len(content)
        
        # An치lise simples de t칩picos (palavras mais frequentes)
        word_freq = {}
        for word in words:
            word = word.lower().strip('.,!?;:"()[]{}')
            if len(word) > 3:  # Ignorar palavras muito curtas
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # Top 5 palavras mais frequentes como t칩picos
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        key_topics = [word for word, freq in top_words]
        
        return {
            "word_count": word_count,
            "character_count": char_count,
            "estimated_reading_time": word_count // 200,  # 200 palavras por minuto
            "summary": f"Documento com {word_count} palavras sobre {', '.join(key_topics[:3])}",
            "key_topics": key_topics,
            "analysis_type": "basic"
        }
    
    def _simple_qa_fallback(self, content: str, question: str) -> str:
        """Fallback simples para QA quando LLM n칚o est치 dispon칤vel"""
        # Busca simples por palavras-chave da pergunta no conte칰do
        question_words = question.lower().split()
        content_lower = content.lower()
        
        # Encontrar senten칞as que cont칡m palavras da pergunta
        sentences = content.split('.')
        relevant_sentences = []
        
        for sentence in sentences[:50]:  # Limitar busca
            sentence_lower = sentence.lower()
            matches = sum(1 for word in question_words if word in sentence_lower)
            if matches >= 2:  # Pelo menos 2 palavras da pergunta
                relevant_sentences.append(sentence.strip())
        
        if relevant_sentences:
            return f"Baseado no documento: {' '.join(relevant_sentences[:2])}"
        else:
            return "N칚o foi poss칤vel encontrar informa칞칚o espec칤fica sobre essa pergunta no documento."
    
    def get_llm_info(self) -> Dict[str, Any]:
        """Retorna informa칞칫es sobre o LLM em uso"""
        if self.llm_manager:
            return self.llm_manager.get_info()
        else:
            return {
                "provider": "none",
                "available": False,
                "reason": "LLM local n칚o inicializado"
            }
    
    def batch_process(self, pdf_paths: List[str]) -> List[Dict[str, Any]]:
        """Processa m칰ltiplos PDFs"""
        results = []
        for pdf_path in pdf_paths:
            result = self.process_pdf(pdf_path)
            results.append(result)
        return results
