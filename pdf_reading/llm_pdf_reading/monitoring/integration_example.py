#!/usr/bin/env python3
"""
Exemplo completo de integra√ß√£o do sistema de monitoramento
com c√≥digo existente de LLM PDF Reading

Este arquivo demonstra como aplicar os decoradores de monitoramento
aos seus c√≥digos existentes sem modificar a l√≥gica principal.
"""

import sys
import time
import random
from pathlib import Path
from typing import Dict, Any, Optional

# Adicionar path do projeto
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Importar decoradores de monitoramento
from llm_pdf_reading.monitoring.decorators import (
    monitor_llm_operation,
    monitor_pdf_operation,
    monitor_qa_operation,
    monitor_complete_operation
)

# Iniciar sistema de monitoramento
from llm_pdf_reading.monitoring import model_monitor
if not model_monitor.is_monitoring:
    model_monitor.start_monitoring()
    print("üöÄ Sistema de monitoramento iniciado!")

# ====================================================================
# EXEMPLO 1: Simula√ß√£o de LocalLLMManager com monitoramento
# ====================================================================

class LocalLLMManagerMonitored:
    """Exemplo de LocalLLMManager com monitoramento integrado"""
    
    def __init__(self):
        self.available_models = [
            "llama2:7b",
            "llama2:13b", 
            "mistral:7b",
            "codellama:7b",
            "gemma:7b"
        ]
        print("ü§ñ LocalLLMManager iniciado com modelos:", self.available_models)
    
    @monitor_llm_operation(
        model_name="llama2:7b",
        provider="ollama"
    )
    def generate_with_ollama(self, prompt: str, model_name: str = "llama2:7b") -> str:
        """
        Simula gera√ß√£o de texto com Ollama
        O decorator automaticamente monitora:
        - Tempo de execu√ß√£o
        - Sucesso/falha
        - M√©tricas LLM
        - Log no MLflow
        """
        print(f"üìù Gerando resposta com {model_name}...")
        
        # Simular processamento (seu c√≥digo existente aqui)
        processing_time = random.uniform(0.5, 3.0)  # Simular varia√ß√£o
        time.sleep(processing_time)
        
        # Simular poss√≠vel falha (5% de chance)
        if random.random() < 0.05:
            raise Exception(f"Erro simulado no modelo {model_name}")
        
        # Retornar resposta simulada
        response = f"""
        Baseado no prompt: "{prompt}"
        
        Esta √© uma resposta simulada do modelo {model_name} via Ollama.
        A resposta foi processada em {processing_time:.2f} segundos.
        
        Conte√∫do da resposta gerada pelo modelo...
        """
        
        print(f"‚úÖ Resposta gerada com sucesso!")
        return response.strip()
    
    @monitor_llm_operation(
        model_name="microsoft/DialoGPT-medium",
        provider="huggingface"
    )
    def generate_with_huggingface(self, prompt: str, model_name: str = "microsoft/DialoGPT-medium") -> str:
        """
        Simula gera√ß√£o de texto com Hugging Face
        Tamb√©m automaticamente monitorado
        """
        print(f"ü§ó Gerando resposta com HuggingFace {model_name}...")
        
        # Simular processamento diferente
        processing_time = random.uniform(1.0, 4.0)
        time.sleep(processing_time)
        
        # Simular falha ocasional
        if random.random() < 0.1:
            raise Exception(f"Erro de conectividade com HuggingFace")
        
        response = f"Resposta do HuggingFace {model_name}: {prompt}"
        print(f"‚úÖ HuggingFace resposta gerada!")
        return response

# ====================================================================
# EXEMPLO 2: Simula√ß√£o de PDFOrchestrator com monitoramento
# ====================================================================

class PDFOrchestratorMonitored:
    """Exemplo de PDFOrchestrator com monitoramento integrado"""
    
    def __init__(self, llm_manager: LocalLLMManagerMonitored):
        self.llm_manager = llm_manager
        print("üìÑ PDFOrchestrator iniciado!")
    
    @monitor_pdf_operation()
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """
        Simula extra√ß√£o de texto do PDF
        Monitora automaticamente:
        - Tempo de processamento
        - Tamanho do arquivo
        - N√∫mero de p√°ginas
        - M√©todo de extra√ß√£o
        """
        print(f"üìñ Extraindo texto de: {pdf_path}")
        
        # Simular extra√ß√£o (seu c√≥digo PyMuPDF aqui)
        extraction_time = random.uniform(0.3, 2.0)
        time.sleep(extraction_time)
        
        # Simular falha ocasional
        if random.random() < 0.02:
            raise Exception(f"Erro ao ler arquivo PDF: {pdf_path}")
        
        # Texto simulado extra√≠do
        extracted_text = f"""
        TEXTO EXTRA√çDO DO PDF: {pdf_path}
        
        Cap√≠tulo 1: Introdu√ß√£o
        Este √© um documento de exemplo que foi processado pelo sistema de 
        leitura de PDFs com monitoramento integrado.
        
        Cap√≠tulo 2: Desenvolvimento  
        O sistema utiliza PyMuPDF para extra√ß√£o de texto e modelos LLM
        para an√°lise e resposta a perguntas.
        
        Cap√≠tulo 3: Conclus√£o
        A integra√ß√£o do monitoramento permite acompanhar a performance
        em tempo real sem modificar o c√≥digo principal.
        
        Total de p√°ginas processadas: 10
        Tempo de extra√ß√£o: {extraction_time:.2f}s
        """
        
        print(f"‚úÖ Texto extra√≠do: {len(extracted_text)} caracteres")
        return extracted_text.strip()
    
    @monitor_qa_operation()
    def answer_question(self, question: str, context: str, model_name: str = "llama2:7b") -> str:
        """
        Simula resposta a pergunta com contexto
        Monitora automaticamente:
        - Qualidade da resposta
        - Tempo de processamento
        - M√©tricas de Q&A
        """
        print(f"‚ùì Respondendo pergunta: {question[:50]}...")
        
        # Usar LLM monitorado
        prompt = f"""
        Contexto: {context[:500]}...
        
        Pergunta: {question}
        
        Por favor, responda baseado no contexto fornecido.
        """
        
        # Chamar LLM (que j√° est√° monitorado)
        if "huggingface" in model_name.lower():
            answer = self.llm_manager.generate_with_huggingface(prompt, model_name)
        else:
            answer = self.llm_manager.generate_with_ollama(prompt, model_name)
        
        print(f"‚úÖ Pergunta respondida!")
        return answer
    
    @monitor_complete_operation(
        operation_type="pdf_qa_pipeline"
    )
    def process_pdf_and_answer(self, pdf_path: str, question: str, model_name: str = "llama2:7b") -> Dict[str, Any]:
        """
        Pipeline completo: PDF ‚Üí Texto ‚Üí Resposta
        Monitora a opera√ß√£o completa end-to-end
        """
        print(f"üîÑ Iniciando pipeline completo...")
        
        result = {
            "pdf_path": pdf_path,
            "question": question, 
            "model_name": model_name,
            "timestamp": time.time()
        }
        
        try:
            # Passo 1: Extrair texto
            print("üìÑ Passo 1: Extraindo texto do PDF...")
            extracted_text = self.extract_text_from_pdf(pdf_path)
            result["extracted_text"] = extracted_text
            result["extraction_success"] = True
            
            # Passo 2: Responder pergunta
            print("ü§ñ Passo 2: Gerando resposta...")
            answer = self.answer_question(question, extracted_text, model_name)
            result["answer"] = answer
            result["qa_success"] = True
            
            # M√©tricas finais
            result["total_time"] = time.time() - result["timestamp"]
            result["success"] = True
            
            print(f"üéâ Pipeline conclu√≠do com sucesso em {result['total_time']:.2f}s!")
            
        except Exception as e:
            result["error"] = str(e)
            result["success"] = False
            result["total_time"] = time.time() - result["timestamp"]
            print(f"‚ùå Erro no pipeline: {e}")
        
        return result

# ====================================================================
# EXEMPLO 3: Simula√ß√£o de Streamlit App monitorado
# ====================================================================

@monitor_complete_operation(operation_type="streamlit_session")
def simulate_streamlit_interaction(pdf_file: str, user_question: str, selected_model: str) -> Dict[str, Any]:
    """
    Simula uma intera√ß√£o completa do usu√°rio no Streamlit
    """
    print(f"üñ•Ô∏è Simulando sess√£o Streamlit...")
    print(f"   üìÅ Arquivo: {pdf_file}")
    print(f"   ‚ùì Pergunta: {user_question}")
    print(f"   ü§ñ Modelo: {selected_model}")
    
    # Inicializar componentes
    llm_manager = LocalLLMManagerMonitored()
    orchestrator = PDFOrchestratorMonitored(llm_manager)
    
    # Processar com monitoramento completo
    result = orchestrator.process_pdf_and_answer(
        pdf_path=pdf_file,
        question=user_question,
        model_name=selected_model
    )
    
    return result

# ====================================================================
# EXEMPLO 4: Testes e Demonstra√ß√£o
# ====================================================================

def test_llm_monitoring():
    """Testa monitoramento de LLM isoladamente"""
    print("\n" + "="*60)
    print("üß™ TESTE 1: Monitoramento de LLM")
    print("="*60)
    
    llm = LocalLLMManagerMonitored()
    
    # Testar m√∫ltiplos modelos
    test_cases = [
        ("llama2:7b", "ollama", "Qual √© a capital do Brasil?"),
        ("llama2:13b", "ollama", "Explique machine learning de forma simples"),
        ("microsoft/DialoGPT-medium", "huggingface", "Como funciona um transformer?")
    ]
    
    for model, provider, prompt in test_cases:
        try:
            print(f"\nüîç Testando {model} via {provider}")
            
            if provider == "ollama":
                response = llm.generate_with_ollama(prompt, model)
            else:
                response = llm.generate_with_huggingface(prompt, model)
                
            print(f"üìù Resposta obtida: {len(response)} caracteres")
            
        except Exception as e:
            print(f"‚ùå Erro: {e}")

def test_pdf_monitoring():
    """Testa monitoramento de PDF isoladamente"""
    print("\n" + "="*60)
    print("üß™ TESTE 2: Monitoramento de PDF")
    print("="*60)
    
    llm = LocalLLMManagerMonitored()
    orchestrator = PDFOrchestratorMonitored(llm)
    
    # Testar diferentes PDFs
    test_pdfs = [
        "manual_usuario.pdf",
        "relatorio_anual.pdf", 
        "artigo_cientifico.pdf"
    ]
    
    for pdf_file in test_pdfs:
        try:
            print(f"\nüìÑ Processando: {pdf_file}")
            text = orchestrator.extract_text_from_pdf(pdf_file)
            print(f"‚úÖ Texto extra√≠do: {len(text)} caracteres")
            
        except Exception as e:
            print(f"‚ùå Erro: {e}")

def test_qa_monitoring():
    """Testa monitoramento de Q&A"""
    print("\n" + "="*60)
    print("üß™ TESTE 3: Monitoramento de Q&A")
    print("="*60)
    
    llm = LocalLLMManagerMonitored()
    orchestrator = PDFOrchestratorMonitored(llm)
    
    # Contexto simulado
    context = """
    O sistema de monitoramento LLM PDF Reading foi desenvolvido para
    fornecer observabilidade completa sobre opera√ß√µes de processamento
    de documentos e gera√ß√£o de respostas. Ele inclui m√©tricas de
    performance, rastreamento de experimentos com MLflow, e alertas
    em tempo real.
    """
    
    # Perguntas de teste
    questions = [
        "O que √© o sistema de monitoramento?",
        "Quais funcionalidades est√£o inclu√≠das?",
        "Como funciona o MLflow?",
    ]
    
    for question in questions:
        try:
            print(f"\n‚ùì Pergunta: {question}")
            answer = orchestrator.answer_question(question, context)
            print(f"‚úÖ Resposta gerada")
            
        except Exception as e:
            print(f"‚ùå Erro: {e}")

def test_complete_pipeline():
    """Testa pipeline completo"""
    print("\n" + "="*60)
    print("üß™ TESTE 4: Pipeline Completo")
    print("="*60)
    
    # Cen√°rios de teste
    scenarios = [
        {
            "pdf": "documento_tecnico.pdf",
            "question": "Quais s√£o as principais conclus√µes do documento?",
            "model": "llama2:7b"
        },
        {
            "pdf": "manual_instrucoes.pdf", 
            "question": "Como configurar o sistema?",
            "model": "mistral:7b"
        },
        {
            "pdf": "relatorio_financeiro.pdf",
            "question": "Qual foi o resultado do trimestre?",
            "model": "microsoft/DialoGPT-medium"
        }
    ]
    
    for i, scenario in enumerate(scenarios, 1):
        print(f"\nüé¨ Cen√°rio {i}:")
        try:
            result = simulate_streamlit_interaction(
                scenario["pdf"],
                scenario["question"], 
                scenario["model"]
            )
            
            if result["success"]:
                print(f"‚úÖ Pipeline executado com sucesso em {result['total_time']:.2f}s")
            else:
                print(f"‚ùå Pipeline falhou: {result.get('error', 'Erro desconhecido')}")
                
        except Exception as e:
            print(f"‚ùå Erro inesperado: {e}")

def show_monitoring_status():
    """Mostra status atual do monitoramento"""
    print("\n" + "="*60)
    print("üìä STATUS DO MONITORAMENTO")
    print("="*60)
    
    from llm_pdf_reading.monitoring import model_monitor
    
    # Status geral
    status = model_monitor.get_system_status()
    print(f"üü¢ Monitoramento ativo: {status['monitoring_active']}")
    print(f"‚è±Ô∏è Tempo ativo: {status['uptime_formatted']}")
    print(f"üìä Disponibilidade: {status['availability']:.1%}")
    print(f"‚ö° Tempo m√©dio: {status['avg_response_time']:.2f}s")
    print(f"üìà Total de requisi√ß√µes: {status['total_requests']}")
    
    # M√©tricas em tempo real
    metrics = model_monitor.get_real_time_metrics()
    print(f"\nü§ñ Modelos monitorados: {len(metrics['model_states'])}")
    
    for model_name, state in metrics['model_states'].items():
        print(f"   - {model_name}: {state['total_requests']} req, {state['avg_response_time']:.2f}s avg")

def main():
    """Fun√ß√£o principal de demonstra√ß√£o"""
    print("üöÄ DEMONSTRA√á√ÉO DE INTEGRA√á√ÉO COM C√ìDIGO EXISTENTE")
    print("="*70)
    print("Este exemplo mostra como integrar o monitoramento aos seus")
    print("c√≥digos existentes usando decoradores.")
    print("="*70)
    
    try:
        # Executar testes sequenciais
        test_llm_monitoring()
        test_pdf_monitoring() 
        test_qa_monitoring()
        test_complete_pipeline()
        
        # Mostrar status final
        show_monitoring_status()
        
        print("\n" + "="*70)
        print("üéâ DEMONSTRA√á√ÉO CONCLU√çDA COM SUCESSO!")
        print("="*70)
        print("\nüìã Como aplicar ao seu c√≥digo:")
        print("1. Importe os decoradores:")
        print("   from llm_pdf_reading.monitoring.decorators import monitor_llm_operation")
        print("\n2. Aplique aos seus m√©todos:")
        print("   @monitor_llm_operation(model_name='seu_modelo', provider='ollama')")
        print("   def sua_funcao_existente(...):")
        print("       # C√≥digo inalterado")
        print("       return resultado")
        print("\n3. Inicie o monitoramento:")
        print("   from llm_pdf_reading.monitoring import model_monitor")
        print("   model_monitor.start_monitoring()")
        print("\n4. Veja os resultados:")
        print("   - Dashboard: streamlit run dashboard.py")
        print("   - MLflow: mlflow ui")
        print("   - Status: model_monitor.get_system_status()")
        
    except Exception as e:
        print(f"\n‚ùå Erro na demonstra√ß√£o: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
