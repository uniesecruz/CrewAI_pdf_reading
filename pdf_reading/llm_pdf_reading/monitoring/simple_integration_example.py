#!/usr/bin/env python3
"""
Exemplo simples de integra√ß√£o com c√≥digo existente
"""

import sys
import time
import random
from pathlib import Path

# Adicionar path do projeto
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Importar decoradores de monitoramento
from llm_pdf_reading.monitoring.decorators import (
    monitor_llm_operation,
    monitor_pdf_operation,
    monitor_complete_operation
)

# Iniciar sistema de monitoramento
from llm_pdf_reading.monitoring import model_monitor

def init_monitoring():
    """Inicializa o sistema de monitoramento"""
    if not model_monitor.is_monitoring:
        model_monitor.start_monitoring()
        print("üöÄ Sistema de monitoramento iniciado!")
    else:
        print("‚úÖ Sistema de monitoramento j√° ativo!")

# ====================================================================
# EXEMPLO 1: Fun√ß√£o LLM Simples com Monitoramento
# ====================================================================

@monitor_llm_operation(
    model_name="llama2:7b",
    provider="ollama"
)
def generate_text_ollama(prompt: str) -> str:
    """
    Exemplo de fun√ß√£o que gera texto com Ollama
    O decorador automaticamente monitora:
    - Tempo de execu√ß√£o
    - Sucesso/falha
    - M√©tricas LLM
    - Log no MLflow
    """
    print(f"ü§ñ Gerando texto com Ollama para: {prompt[:50]}...")
    
    # Simular processamento
    processing_time = random.uniform(0.5, 2.0)
    time.sleep(processing_time)
    
    # Simular poss√≠vel falha (5% de chance)
    if random.random() < 0.05:
        raise Exception("Erro simulado na gera√ß√£o")
    
    response = f"Resposta do Ollama para '{prompt}' (processado em {processing_time:.2f}s)"
    print(f"‚úÖ Texto gerado com sucesso!")
    return response

@monitor_llm_operation(
    model_name="gpt-3.5-turbo",
    provider="openai"
)
def generate_text_openai(prompt: str) -> str:
    """
    Exemplo de fun√ß√£o que gera texto com OpenAI
    """
    print(f"üß† Gerando texto com OpenAI para: {prompt[:50]}...")
    
    # Simular processamento diferente
    processing_time = random.uniform(0.3, 1.5)
    time.sleep(processing_time)
    
    # Simular falha ocasional (3% de chance)
    if random.random() < 0.03:
        raise Exception("Rate limit exceeded")
    
    response = f"Resposta do OpenAI para '{prompt}' (processado em {processing_time:.2f}s)"
    print(f"‚úÖ OpenAI resposta gerada!")
    return response

# ====================================================================
# EXEMPLO 2: Fun√ß√£o PDF Simples com Monitoramento
# ====================================================================

@monitor_pdf_operation()
def extract_pdf_text(pdf_path: str) -> str:
    """
    Exemplo de fun√ß√£o que extrai texto de PDF
    O decorador monitora automaticamente a opera√ß√£o
    """
    print(f"üìÑ Extraindo texto de: {pdf_path}")
    
    # Simular extra√ß√£o
    extraction_time = random.uniform(0.2, 1.0)
    time.sleep(extraction_time)
    
    # Simular falha ocasional (2% de chance)
    if random.random() < 0.02:
        raise Exception(f"Erro ao ler PDF: {pdf_path}")
    
    extracted_text = f"""
    Texto extra√≠do do arquivo: {pdf_path}
    
    Este √© um documento de exemplo processado pelo sistema.
    O texto foi extra√≠do em {extraction_time:.2f} segundos.
    
    Conte√∫do simulado do documento...
    - Se√ß√£o 1: Introdu√ß√£o
    - Se√ß√£o 2: Desenvolvimento  
    - Se√ß√£o 3: Conclus√£o
    """
    
    print(f"‚úÖ Texto extra√≠do: {len(extracted_text)} caracteres")
    return extracted_text.strip()

# ====================================================================
# EXEMPLO 3: Pipeline Completo com Monitoramento
# ====================================================================

@monitor_complete_operation(operation_type="pdf_qa_complete")
def process_document_qa(pdf_path: str, question: str, model_provider: str = "ollama") -> dict:
    """
    Pipeline completo: PDF ‚Üí Texto ‚Üí Resposta
    O decorador monitora toda a opera√ß√£o end-to-end
    """
    print(f"üîÑ Iniciando pipeline: {pdf_path} + '{question}' via {model_provider}")
    
    result = {
        "pdf_path": pdf_path,
        "question": question,
        "model_provider": model_provider,
        "start_time": time.time()
    }
    
    try:
        # Passo 1: Extrair texto do PDF
        print("üìñ Passo 1: Extraindo texto...")
        extracted_text = extract_pdf_text(pdf_path)
        result["extracted_text"] = extracted_text[:200] + "..." # Resumo
        result["extraction_success"] = True
        
        # Passo 2: Gerar resposta
        print("ü§ñ Passo 2: Gerando resposta...")
        prompt = f"Baseado no texto: {extracted_text[:500]}...\n\nPergunta: {question}"
        
        if model_provider == "openai":
            answer = generate_text_openai(prompt)
        else:
            answer = generate_text_ollama(prompt)
            
        result["answer"] = answer
        result["qa_success"] = True
        result["success"] = True
        
        # Calcular tempo total
        result["total_time"] = time.time() - result["start_time"]
        
        print(f"üéâ Pipeline conclu√≠do com sucesso em {result['total_time']:.2f}s!")
        
    except Exception as e:
        result["error"] = str(e)
        result["success"] = False
        result["total_time"] = time.time() - result["start_time"]
        print(f"‚ùå Erro no pipeline: {e}")
    
    return result

# ====================================================================
# TESTES E DEMONSTRA√á√ÉO
# ====================================================================

def test_llm_monitoring():
    """Testa monitoramento de LLM"""
    print("\n" + "="*50)
    print("üß™ TESTE: Monitoramento de LLM")
    print("="*50)
    
    test_prompts = [
        "Qual √© a capital do Brasil?",
        "Explique o que √© intelig√™ncia artificial",
        "Como funciona um algoritmo de machine learning?"
    ]
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nüìù Teste {i}: {prompt}")
        
        try:
            # Testar Ollama
            response_ollama = generate_text_ollama(prompt)
            print(f"   Ollama: {len(response_ollama)} caracteres")
            
            # Testar OpenAI (1 em 3 testes)
            if i % 3 == 0:
                response_openai = generate_text_openai(prompt)
                print(f"   OpenAI: {len(response_openai)} caracteres")
                
        except Exception as e:
            print(f"   ‚ùå Erro: {e}")

def test_pdf_monitoring():
    """Testa monitoramento de PDF"""
    print("\n" + "="*50)
    print("üß™ TESTE: Monitoramento de PDF")
    print("="*50)
    
    test_files = [
        "documento1.pdf",
        "relatorio.pdf", 
        "manual.pdf"
    ]
    
    for pdf_file in test_files:
        print(f"\nüìÑ Processando: {pdf_file}")
        try:
            text = extract_pdf_text(pdf_file)
            print(f"   ‚úÖ Sucesso: {len(text)} caracteres")
        except Exception as e:
            print(f"   ‚ùå Erro: {e}")

def test_complete_pipeline():
    """Testa pipeline completo"""
    print("\n" + "="*50)
    print("üß™ TESTE: Pipeline Completo")
    print("="*50)
    
    scenarios = [
        {
            "pdf": "documento_tecnico.pdf",
            "question": "Quais s√£o as principais conclus√µes?",
            "provider": "ollama"
        },
        {
            "pdf": "relatorio_vendas.pdf",
            "question": "Qual foi o resultado do trimestre?",
            "provider": "openai"
        }
    ]
    
    for i, scenario in enumerate(scenarios, 1):
        print(f"\nüé¨ Cen√°rio {i}: {scenario['pdf']}")
        try:
            result = process_document_qa(
                scenario["pdf"],
                scenario["question"],
                scenario["provider"]
            )
            
            if result["success"]:
                print(f"   ‚úÖ Sucesso em {result['total_time']:.2f}s")
            else:
                print(f"   ‚ùå Falhou: {result.get('error', 'Erro desconhecido')}")
                
        except Exception as e:
            print(f"   üí• Erro inesperado: {e}")

def show_monitoring_summary():
    """Mostra resumo do monitoramento"""
    print("\n" + "="*50)
    print("üìä RESUMO DO MONITORAMENTO")
    print("="*50)
    
    try:
        # Status geral
        status = model_monitor.get_system_status()
        print(f"üìà Total de requisi√ß√µes: {status['total_requests']}")
        print(f"üìä Disponibilidade: {status['availability']:.1%}")
        print(f"‚ö° Tempo m√©dio: {status['avg_response_time']:.2f}s")
        print(f"‚è±Ô∏è Tempo ativo: {status['uptime_formatted']}")
        
        # Modelos monitorados
        metrics = model_monitor.get_real_time_metrics()
        print(f"\nü§ñ Modelos monitorados: {len(metrics['model_states'])}")
        
        for model_name, state in metrics['model_states'].items():
            if state['total_requests'] > 0:
                print(f"   - {model_name}: {state['total_requests']} req, {state['avg_response_time']:.2f}s")
                
    except Exception as e:
        print(f"‚ùå Erro ao obter status: {e}")

def main():
    """Fun√ß√£o principal de demonstra√ß√£o"""
    print("üöÄ EXEMPLO DE INTEGRA√á√ÉO COM C√ìDIGO EXISTENTE")
    print("="*60)
    print("Este exemplo mostra como adicionar monitoramento")
    print("ao seu c√≥digo existente usando apenas decoradores!")
    print("="*60)
    
    # Inicializar monitoramento
    init_monitoring()
    
    try:
        # Executar testes
        test_llm_monitoring()
        test_pdf_monitoring()
        test_complete_pipeline()
        
        # Mostrar resumo
        show_monitoring_summary()
        
        print("\n" + "="*60)
        print("üéâ DEMONSTRA√á√ÉO CONCLU√çDA!")
        print("="*60)
        print("\nüìã Para aplicar ao seu c√≥digo:")
        print("1. Adicione o decorador:")
        print("   @monitor_llm_operation(model_name='seu_modelo', provider='ollama')")
        print("   def sua_funcao(...):")
        print("       return resultado")
        print("\n2. Inicie o monitoramento:")
        print("   from llm_pdf_reading.monitoring import model_monitor")
        print("   model_monitor.start_monitoring()")
        print("\n3. Visualize os resultados:")
        print("   - Dashboard: streamlit run dashboard.py")
        print("   - MLflow: mlflow ui")
        print("   - Status: model_monitor.get_system_status()")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Erro na demonstra√ß√£o: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\n‚ú® Teste de integra√ß√£o executado com sucesso!")
    else:
        print("\nüí• Falha no teste de integra√ß√£o!")
