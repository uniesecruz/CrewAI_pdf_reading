#!/usr/bin/env python3
"""
Teste espec√≠fico para o arquivo Random_Forest.pdf
Demonstra o monitoramento de opera√ß√µes PDF em tempo real
"""

import sys
import os
import time
import pandas as pd
from pathlib import Path
from typing import Dict, Any

# Adicionar path do projeto
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Importar decoradores de monitoramento
from llm_pdf_reading.monitoring.decorators import monitor_pdf_operation
from llm_pdf_reading.monitoring import model_monitor
import mlflow
import mlflow.sklearn

def ask_user_about_document_theme():
    """
    Pergunta ao usu√°rio sobre o tema esperado do documento
    """
    print("\n" + "="*60)
    print("‚ùì PERGUNTA SOBRE O TEMA DO DOCUMENTO")
    print("="*60)
    
    themes_options = {
        "1": "Machine Learning",
        "2": "Random Forest",
        "3": "Data Science", 
        "4": "Deep Learning",
        "5": "Natural Language Processing",
        "6": "Computer Vision",
        "7": "Statistics",
        "8": "Programming",
        "9": "Outro"
    }
    
    print("üéØ Qual tema voc√™ espera encontrar neste documento?")
    print("\nOp√ß√µes dispon√≠veis:")
    for key, value in themes_options.items():
        print(f"   {key}. {value}")
    
    while True:
        try:
            choice = input("\nüìù Digite o n√∫mero da op√ß√£o (1-9): ").strip()
            
            if choice in themes_options:
                expected_theme = themes_options[choice]
                
                if choice == "9":  # Outro
                    custom_theme = input("üìù Digite o tema espec√≠fico: ").strip()
                    if custom_theme:
                        expected_theme = custom_theme
                
                print(f"‚úÖ Tema esperado registrado: {expected_theme}")
                return expected_theme
            else:
                print("‚ùå Op√ß√£o inv√°lida. Digite um n√∫mero de 1 a 9.")
                
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è Opera√ß√£o cancelada pelo usu√°rio.")
            return "N√£o informado"
        except Exception as e:
            print(f"‚ùå Erro: {e}")
            return "Erro na entrada"

def compare_themes_with_mlflow(expected_theme: str, detected_theme: str, confidence: float):
    """
    Compara tema esperado vs detectado e registra no MLflow
    """
    print(f"\nüîç COMPARA√á√ÉO DE TEMAS:")
    print(f"   üéØ Esperado: {expected_theme}")
    print(f"   ü§ñ Detectado: {detected_theme}")
    print(f"   üìà Confian√ßa: {confidence:.1f}%")
    
    # Calcular score de correspond√™ncia
    if expected_theme.lower() in detected_theme.lower() or detected_theme.lower() in expected_theme.lower():
        theme_match_score = 100.0
        match_status = "Exato"
    elif any(word in detected_theme.lower() for word in expected_theme.lower().split()):
        theme_match_score = 75.0
        match_status = "Parcial"
    else:
        theme_match_score = 0.0
        match_status = "Diferente"
    
    print(f"   üéØ Correspond√™ncia: {match_status} ({theme_match_score}%)")
    
    # Registrar no MLflow
    try:
        with mlflow.start_run(nested=True, run_name="theme_comparison"):
            mlflow.log_param("expected_theme", expected_theme)
            mlflow.log_param("detected_theme", detected_theme)
            mlflow.log_param("match_status", match_status)
            mlflow.log_metric("detection_confidence", confidence)
            mlflow.log_metric("theme_match_score", theme_match_score)
            mlflow.log_metric("accuracy_score", (confidence + theme_match_score) / 2)
    except Exception as e:
        print(f"Erro ao iniciar execu√ß√£o MLflow: {e}")
    
    return {
        "expected_theme": expected_theme,
        "detected_theme": detected_theme,
        "match_status": match_status,
        "theme_match_score": theme_match_score,
        "detection_confidence": confidence,
        "accuracy_score": (confidence + theme_match_score) / 2
    }

def init_monitoring():
    """Inicializa o sistema de monitoramento"""
    if not model_monitor.is_monitoring:
        model_monitor.start_monitoring()
        print("üöÄ Sistema de monitoramento iniciado!")
    else:
        print("‚úÖ Sistema de monitoramento j√° ativo!")
    """Inicializa o sistema de monitoramento"""
    if not model_monitor.is_monitoring:
        model_monitor.start_monitoring()
        print("üöÄ Sistema de monitoramento iniciado!")
    else:
        print("‚úÖ Sistema de monitoramento j√° ativo!")

@monitor_pdf_operation()
def extract_text_from_random_forest_pdf(pdf_path: str) -> Dict[str, Any]:
    """
    Extrai texto do Random_Forest.pdf com monitoramento autom√°tico
    """
    print(f"üìÑ Iniciando extra√ß√£o do Random Forest PDF: {pdf_path}")
    
    start_time = time.time()
    
    try:
        # Verificar se o arquivo existe
        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"Arquivo n√£o encontrado: {pdf_path}")
        
        # Obter informa√ß√µes do arquivo
        file_size = os.path.getsize(pdf_path)
        print(f"üìä Tamanho do arquivo: {file_size / 1024:.2f} KB")
        
        # Tentar usar PyMuPDF para extra√ß√£o real
        try:
            import fitz  # PyMuPDF
            print("‚úÖ PyMuPDF dispon√≠vel - extra√ß√£o avan√ßada")
            
            # Abrir o PDF
            doc = fitz.open(pdf_path)
            num_pages = len(doc)
            print(f"üìñ N√∫mero de p√°ginas: {num_pages}")
            
            # Extrair texto das primeiras p√°ginas
            full_text = ""
            pages_to_extract = min(num_pages, 5)  # Limitar para performance
            
            for page_num in range(pages_to_extract):
                page = doc.load_page(page_num)
                page_text = page.get_text()
                full_text += f"\n--- P√°gina {page_num + 1} ---\n{page_text}"
                print(f"   üìÑ P√°gina {page_num + 1}: {len(page_text)} caracteres")
            
            # Extrair metadados
            metadata = doc.metadata
            
            doc.close()
            
            # Calcular estat√≠sticas
            processing_time = time.time() - start_time
            text_length = len(full_text)
            words_count = len(full_text.split())
            
            result = {
                "success": True,
                "file_path": pdf_path,
                "file_size_bytes": file_size,
                "file_size_kb": file_size / 1024,
                "num_pages": num_pages,
                "pages_processed": pages_to_extract,
                "text_length": text_length,
                "words_count": words_count,
                "processing_time": processing_time,
                "metadata": metadata,
                "extracted_text": full_text,
                "summary": f"Extra√≠do {text_length} caracteres de {pages_to_extract}/{num_pages} p√°ginas em {processing_time:.2f}s"
            }
            
            print(f"‚úÖ Extra√ß√£o conclu√≠da com sucesso!")
            print(f"   üìä {text_length} caracteres extra√≠dos")
            print(f"   üìñ {pages_to_extract} de {num_pages} p√°ginas processadas")
            print(f"   üî§ {words_count} palavras encontradas")
            print(f"   ‚è±Ô∏è {processing_time:.2f} segundos")
            
            return result
            
        except ImportError:
            print("‚ö†Ô∏è PyMuPDF n√£o dispon√≠vel - usando extra√ß√£o b√°sica")
            
            # M√©todo alternativo - ler como texto se for poss√≠vel
            try:
                with open(pdf_path, 'rb') as f:
                    content = f.read()
                
                # Simular extra√ß√£o baseada no tamanho
                processing_time = time.time() - start_time
                
                result = {
                    "success": True,
                    "file_path": pdf_path,
                    "file_size_bytes": file_size,
                    "extraction_method": "basic",
                    "processing_time": processing_time,
                    "message": f"Arquivo {file_size} bytes processado (m√©todo b√°sico)"
                }
                
                print(f"‚úÖ Processamento b√°sico conclu√≠do em {processing_time:.2f}s")
                return result
                
            except Exception as e:
                raise Exception(f"Erro na extra√ß√£o b√°sica: {e}")
        
    except Exception as e:
        processing_time = time.time() - start_time
        
        result = {
            "success": False,
            "file_path": pdf_path,
            "error": str(e),
            "processing_time": processing_time,
            "error_type": type(e).__name__
        }
        
        print(f"‚ùå Erro na extra√ß√£o: {e}")
        return result

@monitor_pdf_operation()
def analyze_document_theme(pdf_path: str) -> Dict[str, Any]:
    """
    Analisa o tema principal do documento usando an√°lise de texto
    """
    print(f"üéØ Analisando tema do documento: {pdf_path}")
    
    start_time = time.time()
    
    try:
        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"Arquivo n√£o encontrado: {pdf_path}")
        
        file_size = os.path.getsize(pdf_path)
        
        # Iniciar experimento MLflow
        try:
            with mlflow.start_run(nested=True, run_name="document_theme_analysis"):
                mlflow.log_param("file_path", pdf_path)
                mlflow.log_param("file_size_kb", file_size / 1024)
                
                try:
                    import fitz
                    
                    doc = fitz.open(pdf_path)
                    num_pages = len(doc)
                    mlflow.log_param("num_pages", num_pages)
                    
                    # Extrair texto completo
                    full_text = ""
                    for page_num in range(min(num_pages, 20)):  # Analisar at√© 20 p√°ginas
                        page = doc.load_page(page_num)
                        page_text = page.get_text()
                        full_text += page_text + " "
                    
                    doc.close()
                    
                    # An√°lise de temas t√©cnicos
                    themes = {
                        "Machine Learning": ["machine learning", "ml", "algorithm", "model", "training", "prediction"],
                        "Random Forest": ["random forest", "decision tree", "ensemble", "bagging", "bootstrap"],
                        "Data Science": ["data science", "dataset", "feature", "analysis", "statistical"],
                        "Deep Learning": ["deep learning", "neural network", "tensorflow", "pytorch", "gradient"],
                        "Natural Language": ["nlp", "natural language", "text processing", "sentiment", "tokenization"],
                        "Computer Vision": ["computer vision", "image", "opencv", "cnn", "convolution"],
                        "Statistics": ["statistics", "probability", "distribution", "hypothesis", "correlation"],
                        "Programming": ["python", "programming", "code", "function", "class", "variable"]
                    }
                    
                    text_lower = full_text.lower()
                    theme_scores = {}
                    total_words = len(full_text.split())
                    
                    print(f"   üìä Analisando {total_words} palavras...")
                    
                    for theme, keywords in themes.items():
                        score = 0
                        found_keywords = []
                        
                        for keyword in keywords:
                            count = text_lower.count(keyword)
                            if count > 0:
                                score += count
                                found_keywords.append(f"{keyword}:{count}")
                        
                        if score > 0:
                            theme_scores[theme] = {
                                "score": score,
                                "percentage": (score / max(total_words, 1)) * 100,
                                "keywords_found": found_keywords
                            }
                            
                            # Log m√©tricas no MLflow
                            mlflow.log_metric(f"theme_{theme.lower().replace(' ', '_')}_score", score)
                            mlflow.log_metric(f"theme_{theme.lower().replace(' ', '_')}_percentage", theme_scores[theme]["percentage"])
                    
                    # Determinar tema principal
                    if theme_scores:
                        main_theme = max(theme_scores.keys(), key=lambda x: theme_scores[x]["score"])
                        main_theme_score = theme_scores[main_theme]["score"]
                        confidence = min(theme_scores[main_theme]["percentage"] * 10, 100)
                    else:
                        main_theme = "Tema n√£o identificado"
                        main_theme_score = 0
                        confidence = 0
                    
                    processing_time = time.time() - start_time
                    
                    # Log m√©tricas principais
                    mlflow.log_metric("processing_time_seconds", processing_time)
                    mlflow.log_metric("total_words", total_words)
                    mlflow.log_metric("main_theme_score", main_theme_score)
                    mlflow.log_metric("confidence_percentage", confidence)
                    mlflow.log_param("main_theme", main_theme)
                    
                    result = {
                        "success": True,
                        "file_path": pdf_path,
                        "file_size": file_size,
                        "num_pages": num_pages,
                        "total_words": total_words,
                        "main_theme": main_theme,
                        "confidence": confidence,
                        "theme_scores": theme_scores,
                        "processing_time": processing_time,
                        "analysis_summary": f"Tema principal: {main_theme} (confian√ßa: {confidence:.1f}%)"
                    }
                    
                    print(f"‚úÖ An√°lise de tema conclu√≠da em {processing_time:.2f}s")
                    print(f"   üéØ Tema principal: {main_theme}")
                    print(f"   üìà Confian√ßa: {confidence:.1f}%")
                    print(f"   üîç Temas encontrados:")
                    
                    for theme, data in sorted(theme_scores.items(), key=lambda x: x[1]["score"], reverse=True):
                        print(f"      - {theme}: {data['score']} men√ß√µes ({data['percentage']:.2f}%)")
                    
                    return result
                    
                except ImportError:
                    # An√°lise simulada sem PyMuPDF
                    processing_time = time.time() - start_time
                    
                    # Simular an√°lise baseada no nome do arquivo
                    filename = os.path.basename(pdf_path).lower()
                    
                    if "random_forest" in filename:
                        main_theme = "Random Forest"
                        confidence = 85.0
                        theme_scores = {"Random Forest": {"score": 15, "percentage": 5.2}}
                    elif "machine" in filename or "ml" in filename:
                        main_theme = "Machine Learning"
                        confidence = 75.0
                        theme_scores = {"Machine Learning": {"score": 12, "percentage": 4.1}}
                    else:
                        main_theme = "Documento T√©cnico"
                        confidence = 60.0
                        theme_scores = {"Programming": {"score": 8, "percentage": 2.8}}
                    
                    # Log m√©tricas simuladas
                    mlflow.log_metric("processing_time_seconds", processing_time)
                    mlflow.log_metric("confidence_percentage", confidence)
                    mlflow.log_param("main_theme", main_theme)
                    mlflow.log_param("analysis_method", "simulated")
                    
                    result = {
                        "success": True,
                        "file_path": pdf_path,
                        "file_size": file_size,
                        "main_theme": main_theme,
                        "confidence": confidence,
                        "theme_scores": theme_scores,
                        "processing_time": processing_time,
                        "analysis_method": "simulated",
                        "analysis_summary": f"Tema simulado: {main_theme} (confian√ßa: {confidence:.1f}%)"
                    }
                    
                    print(f"‚úÖ An√°lise simulada conclu√≠da em {processing_time:.2f}s")
                    print(f"   üéØ Tema identificado: {main_theme}")
                    print(f"   üìà Confian√ßa: {confidence:.1f}%")
                    
                    return result
        except Exception as e:
            print(f"Erro ao iniciar execu√ß√£o MLflow: {e}")
        
    except Exception as e:
        processing_time = time.time() - start_time
        
        # Log erro no MLflow
        try:
            with mlflow.start_run(nested=True, run_name="document_theme_analysis_error"):
                mlflow.log_param("error", str(e))
                mlflow.log_metric("processing_time_seconds", processing_time)
                mlflow.log_param("success", False)
        except Exception as mlflow_error:
            print(f"Erro adicional no MLflow: {mlflow_error}")
        
        result = {
            "success": False,
            "file_path": pdf_path,
            "error": str(e),
            "processing_time": processing_time
        }
        
        print(f"‚ùå Erro na an√°lise de tema: {e}")
        return result

@monitor_pdf_operation()
def analyze_random_forest_content(pdf_path: str) -> Dict[str, Any]:
    """
    An√°lise espec√≠fica do conte√∫do sobre Random Forest com MLflow
    """
    print(f"üîç Analisando conte√∫do sobre Random Forest: {pdf_path}")
    
    start_time = time.time()
    
    try:
        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"Arquivo n√£o encontrado: {pdf_path}")
        
        file_size = os.path.getsize(pdf_path)
        
        # Iniciar experimento MLflow para Random Forest
        try:
            with mlflow.start_run(nested=True, run_name="random_forest_content_analysis"):
                mlflow.log_param("file_path", pdf_path)
                mlflow.log_param("file_size_kb", file_size / 1024)
                
                try:
                    import fitz
                    
                    doc = fitz.open(pdf_path)
                    num_pages = len(doc)
                    mlflow.log_param("num_pages", num_pages)
                    
                    # Procurar por termos relacionados a Random Forest
                    rf_terms = {
                        'random_forest': ['random forest', 'randomforest'],
                        'decision_trees': ['decision tree', 'decision trees'],
                        'ensemble': ['ensemble', 'ensemble method'],
                        'bagging': ['bagging', 'bootstrap aggregating'],
                        'bootstrap': ['bootstrap', 'bootstrap sampling'],
                        'feature_importance': ['feature importance', 'variable importance'],
                        'overfitting': ['overfitting', 'overfit'],
                        'accuracy': ['accuracy', 'performance'],
                        'classification': ['classification', 'classifier'],
                        'regression': ['regression', 'regressor']
                    }
                    
                    found_terms = {}
                    full_text = ""
                    
                    for page_num in range(min(num_pages, 15)):  # Analisar at√© 15 p√°ginas
                        page = doc.load_page(page_num)
                        page_text = page.get_text().lower()
                        full_text += page_text
                        
                        # Contar ocorr√™ncias de termos
                        for category, terms in rf_terms.items():
                            for term in terms:
                                count = page_text.count(term)
                                if count > 0:
                                    if category not in found_terms:
                                        found_terms[category] = 0
                                    found_terms[category] += count
                    
                    doc.close()
                    
                    # An√°lise de conte√∫do
                    total_chars = len(full_text)
                    total_words = len(full_text.split())
                    total_rf_mentions = sum(found_terms.values())
                    
                    # Calcular m√©tricas de qualidade
                    content_density = total_rf_mentions / max(total_words, 1) * 100
                    technical_depth = len(found_terms) / len(rf_terms) * 100
                    
                    processing_time = time.time() - start_time
                    
                    # Log todas as m√©tricas no MLflow
                    mlflow.log_metric("processing_time_seconds", processing_time)
                    mlflow.log_metric("total_characters", total_chars)
                    mlflow.log_metric("total_words", total_words)
                    mlflow.log_metric("total_rf_mentions", total_rf_mentions)
                    mlflow.log_metric("content_density_percentage", content_density)
                    mlflow.log_metric("technical_depth_percentage", technical_depth)
                    
                    # Log m√©tricas espec√≠ficas por categoria
                    for category, count in found_terms.items():
                        mlflow.log_metric(f"rf_{category}_mentions", count)
                    
                    result = {
                        "success": True,
                        "file_path": pdf_path,
                        "file_size": file_size,
                        "num_pages": num_pages,
                        "total_characters": total_chars,
                        "total_words": total_words,
                        "rf_terms_found": found_terms,
                        "total_rf_mentions": total_rf_mentions,
                        "content_density": content_density,
                        "technical_depth": technical_depth,
                        "processing_time": processing_time,
                        "analysis_summary": f"Encontrados {total_rf_mentions} men√ß√µes a Random Forest (densidade: {content_density:.2f}%)"
                    }
                    
                    print(f"‚úÖ An√°lise conclu√≠da em {processing_time:.2f}s")
                    print(f"   üìä {total_words} palavras analisadas")
                    print(f"   üå≥ {total_rf_mentions} men√ß√µes a Random Forest")
                    print(f"   üìà Densidade do conte√∫do: {content_density:.2f}%")
                    print(f"   üéØ Profundidade t√©cnica: {technical_depth:.2f}%")
                    
                    if found_terms:
                        print("   üîç Categorias encontradas:")
                        for category, count in found_terms.items():
                            print(f"      - {category.replace('_', ' ').title()}: {count} men√ß√µes")
                    
                    return result
                    
                except ImportError:
                    # An√°lise simulada
                    processing_time = time.time() - start_time
                    
                    # Simular m√©tricas baseadas no arquivo
                    simulated_rf_mentions = 25
                    simulated_content_density = 3.2
                    simulated_technical_depth = 75.0
                    
                    # Log m√©tricas simuladas
                    mlflow.log_metric("processing_time_seconds", processing_time)
                    mlflow.log_metric("total_rf_mentions", simulated_rf_mentions)
                    mlflow.log_metric("content_density_percentage", simulated_content_density)
                    mlflow.log_metric("technical_depth_percentage", simulated_technical_depth)
                    mlflow.log_param("analysis_method", "simulated")
                    
                    result = {
                        "success": True,
                        "file_path": pdf_path,
                        "file_size": file_size,
                        "analysis_type": "simulated",
                        "total_rf_mentions": simulated_rf_mentions,
                        "content_density": simulated_content_density,
                        "technical_depth": simulated_technical_depth,
                        "processing_time": processing_time,
                        "message": "An√°lise simulada - arquivo processado com sucesso"
                    }
                    
                    print(f"‚úÖ An√°lise simulada conclu√≠da em {processing_time:.2f}s")
                    print(f"   üå≥ {simulated_rf_mentions} men√ß√µes simuladas")
                    print(f"   üìà Densidade simulada: {simulated_content_density:.2f}%")
                    
                    return result
        except Exception as e:
            print(f"Erro ao iniciar execu√ß√£o MLflow: {e}")
        
    except Exception as e:
        processing_time = time.time() - start_time
        
        # Log erro no MLflow
        try:
            with mlflow.start_run(nested=True, run_name="random_forest_analysis_error"):
                mlflow.log_param("error", str(e))
                mlflow.log_metric("processing_time_seconds", processing_time)
                mlflow.log_param("success", False)
        except Exception as mlflow_error:
            print(f"Erro adicional no MLflow: {mlflow_error}")
        
        result = {
            "success": False,
            "file_path": pdf_path,
            "error": str(e),
            "processing_time": processing_time
        }
        
        print(f"‚ùå Erro na an√°lise: {e}")
        return result

def show_monitoring_dashboard():
    """
    Mostra dashboard de monitoramento ap√≥s os testes
    """
    print("\n" + "="*70)
    print("üìä DASHBOARD DE MONITORAMENTO - RANDOM FOREST PDF")
    print("="*70)
    
    try:
        # Status do sistema
        status = model_monitor.get_system_status()
        print(f"üéØ M√âTRICAS GERAIS:")
        print(f"   üìà Total de opera√ß√µes: {status.get('total_requests', 0)}")
        print(f"   üìä Taxa de sucesso: {status.get('availability', 1.0):.1%}")
        print(f"   ‚è±Ô∏è Tempo m√©dio de resposta: {status.get('avg_response_time', 0.0):.2f}s")
        
        # Tratar erro de uptime se n√£o existir
        uptime = status.get('uptime', 0)
        if uptime is not None:
            print(f"   ‚è∞ Tempo ativo: {uptime:.0f}s")
        else:
            print(f"   ‚è∞ Tempo ativo: N√£o dispon√≠vel")
        
        # M√©tricas em tempo real
        metrics = model_monitor.get_real_time_metrics()
        
        print(f"\nüîÑ OPERA√á√ïES EM TEMPO REAL:")
        current_ops = metrics.get('current_operations', {})
        if current_ops:
            for op_id, op_data in current_ops.items():
                print(f"   - {op_data['operation']}: {op_data['duration_so_far']:.2f}s")
        else:
            print("   ‚úÖ Nenhuma opera√ß√£o ativa")
        
        # Performance tracker
        from llm_pdf_reading.monitoring import performance_tracker
        summary = performance_tracker.get_performance_summary()
        
        if summary:
            print(f"\n‚ö° RESUMO DE PERFORMANCE:")
            for operation, stats in summary.items():
                print(f"   üìÑ {operation}:")
                print(f"      üî¢ Execu√ß√µes: {stats['count']}")
                print(f"      ‚è±Ô∏è Tempo m√©dio: {stats['avg_duration']:.2f}s")
                print(f"      üìä M√≠n/M√°x: {stats['min_duration']:.2f}s / {stats['max_duration']:.2f}s")
                if stats['count'] > 1:
                    print(f"      üìà √öltima execu√ß√£o: {stats['last_duration']:.2f}s")
        
    except Exception as e:
        print(f"‚ùå Erro ao obter m√©tricas: {e}")
        print("   üí° Continuando com execu√ß√£o normal...")

def main():
    """
    Teste principal com Random_Forest.pdf e an√°lise de tema com MLflow
    """
    print("üå≥ TESTE DE MONITORAMENTO - RANDOM FOREST PDF COM AN√ÅLISE DE TEMA")
    print("="*80)
    print("Testando o sistema de monitoramento com an√°lise de tema e m√©tricas MLflow")
    print("="*80)
    
    # Configurar MLflow
    mlflow.set_tracking_uri("sqlite:///mlflow.db")
    mlflow.set_experiment("Random_Forest_PDF_Analysis")
    
    # Finalizar run ativo se existir
    try:
        active_run = mlflow.active_run()
        if active_run:
            print("‚ö†Ô∏è Finalizando run MLflow ativo...")
            mlflow.end_run()
    except Exception as e:
        print(f"Aviso: {e}")
    
    # Caminho para o arquivo
    pdf_path = r"c:\Users\win\Desktop\Projetos\pdf_reading\pdf_reading\data\external\Random_Forest.pdf"
    
    # Verificar se existe
    if not os.path.exists(pdf_path):
        print(f"‚ùå Arquivo n√£o encontrado: {pdf_path}")
        return False
    
    print(f"üìÅ Arquivo encontrado: {pdf_path}")
    print(f"üìä Tamanho: {os.path.getsize(pdf_path) / 1024:.2f} KB")
    
    # Inicializar monitoramento
    init_monitoring()
    
    # Perguntar ao usu√°rio sobre o tema esperado
    expected_theme = ask_user_about_document_theme()
    
    try:
        # Iniciar experimento principal no MLflow
        with mlflow.start_run(run_name="Complete_PDF_Analysis_with_Theme_Question"):
            # Log informa√ß√µes b√°sicas do arquivo
            mlflow.log_param("pdf_file", os.path.basename(pdf_path))
            mlflow.log_param("file_size_kb", os.path.getsize(pdf_path) / 1024)
            mlflow.log_param("analysis_timestamp", time.strftime("%Y-%m-%d %H:%M:%S"))
            mlflow.log_param("expected_theme", expected_theme)
            
            print("\n" + "="*60)
            print("üß™ TESTE 1: An√°lise de Tema do Documento")
            print("="*60)
            
            # Teste 1: An√°lise de tema
            result_theme = analyze_document_theme(pdf_path)
            
            theme_comparison = None
            if result_theme["success"]:
                print(f"‚úÖ An√°lise de tema bem-sucedida!")
                print(f"   üéØ Tema: {result_theme['main_theme']}")
                print(f"   üìà Confian√ßa: {result_theme['confidence']:.1f}%")
                
                # Comparar com tema esperado
                theme_comparison = compare_themes_with_mlflow(
                    expected_theme, 
                    result_theme['main_theme'], 
                    result_theme['confidence']
                )
                
                # Log m√©tricas principais do tema
                mlflow.log_metric("theme_analysis_success", 1)
                mlflow.log_param("main_theme", result_theme['main_theme'])
                mlflow.log_metric("theme_confidence", result_theme['confidence'])
                mlflow.log_metric("theme_match_score", theme_comparison['theme_match_score'])
                mlflow.log_metric("theme_accuracy", theme_comparison['accuracy_score'])
            else:
                print(f"‚ùå Falha na an√°lise de tema: {result_theme['error']}")
                mlflow.log_metric("theme_analysis_success", 0)
                mlflow.log_param("theme_error", result_theme['error'])
            
            print("\n" + "="*60)
            print("üß™ TESTE 2: Extra√ß√£o de Texto")
            print("="*60)
            
            # Teste 2: Extra√ß√£o de texto
            result1 = extract_text_from_random_forest_pdf(pdf_path)
            
            if result1["success"]:
                print(f"‚úÖ Extra√ß√£o bem-sucedida!")
                mlflow.log_metric("text_extraction_success", 1)
                mlflow.log_metric("extraction_time", result1.get("processing_time", 0))
                
                if "extracted_text" in result1:
                    # Mostrar amostra do texto
                    sample = result1["extracted_text"][:500]
                    print(f"\nüìñ Amostra do conte√∫do extra√≠do:")
                    print("-" * 50)
                    print(sample + "...")
                    print("-" * 50)
                    
                    # Log m√©tricas de extra√ß√£o
                    mlflow.log_metric("extracted_characters", result1.get("text_length", 0))
                    mlflow.log_metric("extracted_words", result1.get("words_count", 0))
                    mlflow.log_metric("pages_processed", result1.get("pages_processed", 0))
                    mlflow.log_metric("total_pages", result1.get("num_pages", 0))
                    
                    # Calcular efici√™ncia de extra√ß√£o
                    if result1.get("num_pages", 0) > 0:
                        extraction_efficiency = (result1.get("pages_processed", 0) / result1.get("num_pages", 1)) * 100
                        mlflow.log_metric("extraction_efficiency_percentage", extraction_efficiency)
            else:
                print(f"‚ùå Falha na extra√ß√£o: {result1['error']}")
                mlflow.log_metric("text_extraction_success", 0)
                mlflow.log_param("extraction_error", result1['error'])
            
            print("\n" + "="*60)
            print("üß™ TESTE 3: An√°lise de Conte√∫do Random Forest")
            print("="*60)
            
            # Teste 3: An√°lise de conte√∫do
            result2 = analyze_random_forest_content(pdf_path)
            
            if result2["success"]:
                print(f"‚úÖ An√°lise bem-sucedida!")
                mlflow.log_metric("rf_analysis_success", 1)
                mlflow.log_metric("rf_analysis_time", result2.get("processing_time", 0))
                
                if "total_rf_mentions" in result2:
                    mlflow.log_metric("total_rf_mentions", result2["total_rf_mentions"])
                    mlflow.log_metric("content_density", result2.get("content_density", 0))
                    mlflow.log_metric("technical_depth", result2.get("technical_depth", 0))
                    
                    # Calcular relev√¢ncia baseada no tema esperado
                    if "random forest" in expected_theme.lower() or "forest" in expected_theme.lower():
                        relevance_score = min(result2.get("content_density", 0) * 10, 100)
                    else:
                        relevance_score = max(50 - result2.get("content_density", 0), 0)
                    
                    mlflow.log_metric("content_relevance_score", relevance_score)
            else:
                print(f"‚ùå Falha na an√°lise: {result2['error']}")
                mlflow.log_metric("rf_analysis_success", 0)
                mlflow.log_param("rf_analysis_error", result2['error'])
            
            # Calcular m√©tricas gerais de qualidade
            total_success = sum([
                1 if result_theme["success"] else 0,
                1 if result1["success"] else 0,
                1 if result2["success"] else 0
            ])
            
            overall_success_rate = (total_success / 3) * 100
            
            # Calcular score de qualidade geral
            quality_score = 0
            if result_theme["success"]:
                quality_score += result_theme.get("confidence", 0) * 0.4
            if result1["success"] and result1.get("words_count", 0) > 0:
                quality_score += min(result1.get("words_count", 0) / 1000 * 100, 100) * 0.3
            if result2["success"]:
                quality_score += result2.get("content_density", 0) * 10 * 0.3
            
            # Log m√©tricas finais
            mlflow.log_metric("overall_success_rate", overall_success_rate)
            mlflow.log_metric("total_tests", 3)
            mlflow.log_metric("successful_tests", total_success)
            mlflow.log_metric("quality_score", quality_score)
            
            # M√©tricas de performance temporal
            total_processing_time = sum([
                result_theme.get("processing_time", 0),
                result1.get("processing_time", 0),
                result2.get("processing_time", 0)
            ])
            mlflow.log_metric("total_processing_time", total_processing_time)
            mlflow.log_metric("avg_processing_time_per_test", total_processing_time / 3)
            
            # Mostrar dashboard
            show_monitoring_dashboard()
            
            # Mostrar resumo MLflow com m√©tricas principais
            show_comprehensive_mlflow_summary(theme_comparison, total_processing_time, quality_score)
            
            print("\n" + "="*80)
            print("üéâ TESTE CONCLU√çDO COM SUCESSO!")
            print("="*80)
            print("\nüìã O que foi analisado e monitorado:")
            print("‚úÖ Pergunta sobre tema esperado do documento")
            print("‚úÖ Compara√ß√£o tema esperado vs detectado")
            print("‚úÖ An√°lise de confian√ßa da detec√ß√£o")
            print("‚úÖ Extra√ß√£o completa de texto")
            print("‚úÖ An√°lise espec√≠fica de Random Forest")
            print("‚úÖ M√©tricas de densidade e relev√¢ncia")
            print("‚úÖ Score de qualidade geral")
            print("‚úÖ M√©tricas temporais de performance")
            print("‚úÖ Taxa de sucesso detalhada")
            
            print(f"\nüìä Resumo das M√©tricas Principais:")
            print(f"   üéØ Taxa de sucesso geral: {overall_success_rate:.1f}%")
            print(f"   üß™ Testes executados: {total_success}/3")
            print(f"   üìà Score de qualidade: {quality_score:.1f}")
            print(f"   ‚è±Ô∏è Tempo total: {total_processing_time:.2f}s")
            if theme_comparison:
                print(f"   üé≠ Correspond√™ncia de tema: {theme_comparison['match_status']} ({theme_comparison['theme_match_score']:.1f}%)")
            
            print(f"\nüîç Pr√≥ximos passos:")
            print(f"   üìä Ver dashboard: streamlit run dashboard.py")
            print(f"   üìà Ver MLflow UI: mlflow ui --port 5001")
            print(f"   üìù Ver logs: model_monitor.get_system_status()")
            print(f"   üóÑÔ∏è Banco MLflow: sqlite:///mlflow.db")
            
            # Finalizar run MLflow
            try:
                if mlflow.active_run():
                    mlflow.end_run()
            except Exception:
                pass
            
            return True
        
    except Exception as e:
        print(f"\n‚ùå Erro no teste: {e}")
        import traceback
        traceback.print_exc()
        
        # Log erro no MLflow
        try:
            with mlflow.start_run(run_name="Complete_PDF_Analysis_Error"):
                mlflow.log_param("error", str(e))
                mlflow.log_param("error_type", type(e).__name__)
                mlflow.log_param("expected_theme", expected_theme)
                mlflow.log_metric("success", 0)
        except Exception as mlflow_error:
            print(f"Erro adicional no MLflow: {mlflow_error}")
        
        return False

def show_comprehensive_mlflow_summary(theme_comparison=None, total_processing_time=0, quality_score=0):
    """
    Mostra resumo abrangente das m√©tricas registradas no MLflow
    """
    print("\n" + "="*70)
    print("üìà RESUMO ABRANGENTE DAS M√âTRICAS MLFLOW")
    print("="*70)
    
    try:
        import mlflow.tracking
        
        # Obter informa√ß√µes do experimento atual
        experiment = mlflow.get_experiment_by_name("Random_Forest_PDF_Analysis")
        if experiment:
            print(f"üß™ Experimento: {experiment.name}")
            print(f"üìÅ ID: {experiment.experiment_id}")
            print(f"üóÑÔ∏è Local: {mlflow.get_tracking_uri()}")
            
            # Listar runs recentes
            runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id], max_results=5)
            
            if not runs.empty:
                print(f"\nüìä √öltima execu√ß√£o (m√©tricas principais):")
                latest_run = runs.iloc[0]
                
                run_name = latest_run.get('tags.mlflow.runName', 'Unnamed')
                status = latest_run['status']
                start_time = latest_run['start_time']
                
                print(f"   üìù Nome: {run_name}")
                print(f"   ‚úÖ Status: {status}")
                print(f"   üïê In√≠cio: {start_time}")
                
                # M√©tricas principais organizadas por categoria
                print(f"\nüéØ M√âTRICAS DE AN√ÅLISE DE TEMA:")
                theme_metrics = {
                    'theme_confidence': 'Confian√ßa da detec√ß√£o',
                    'theme_match_score': 'Score de correspond√™ncia',
                    'theme_accuracy': 'Precis√£o geral do tema'
                }
                
                for metric_key, metric_name in theme_metrics.items():
                    metric_col = f'metrics.{metric_key}'
                    if metric_col in latest_run and not pd.isna(latest_run[metric_col]):
                        value = latest_run[metric_col]
                        print(f"   üìà {metric_name}: {value:.1f}%")
                
                print(f"\nüìÑ M√âTRICAS DE EXTRA√á√ÉO DE TEXTO:")
                extraction_metrics = {
                    'extracted_words': 'Palavras extra√≠das',
                    'extracted_characters': 'Caracteres extra√≠dos',
                    'pages_processed': 'P√°ginas processadas',
                    'extraction_efficiency_percentage': 'Efici√™ncia da extra√ß√£o'
                }
                
                for metric_key, metric_name in extraction_metrics.items():
                    metric_col = f'metrics.{metric_key}'
                    if metric_col in latest_run and not pd.isna(latest_run[metric_col]):
                        value = latest_run[metric_col]
                        if 'percentage' in metric_key or 'efficiency' in metric_key:
                            print(f"   üìä {metric_name}: {value:.1f}%")
                        else:
                            print(f"   üìä {metric_name}: {value:.0f}")
                
                print(f"\nüå≥ M√âTRICAS DE RANDOM FOREST:")
                rf_metrics = {
                    'total_rf_mentions': 'Total de men√ß√µes RF',
                    'content_density': 'Densidade do conte√∫do',
                    'technical_depth': 'Profundidade t√©cnica',
                    'content_relevance_score': 'Score de relev√¢ncia'
                }
                
                for metric_key, metric_name in rf_metrics.items():
                    metric_col = f'metrics.{metric_key}'
                    if metric_col in latest_run and not pd.isna(latest_run[metric_col]):
                        value = latest_run[metric_col]
                        if 'density' in metric_key or 'depth' in metric_key or 'score' in metric_key:
                            print(f"   üîç {metric_name}: {value:.2f}%")
                        else:
                            print(f"   üîç {metric_name}: {value:.0f}")
                
                print(f"\n‚ö° M√âTRICAS DE PERFORMANCE:")
                performance_metrics = {
                    'total_processing_time': f'Tempo total: {total_processing_time:.2f}s',
                    'avg_processing_time_per_test': 'Tempo m√©dio por teste',
                    'overall_success_rate': 'Taxa de sucesso geral',
                    'quality_score': f'Score de qualidade: {quality_score:.1f}'
                }
                
                for metric_key, metric_name in performance_metrics.items():
                    metric_col = f'metrics.{metric_key}'
                    if metric_col in latest_run and not pd.isna(latest_run[metric_col]):
                        value = latest_run[metric_col]
                        if metric_key == 'total_processing_time':
                            print(f"   ‚è±Ô∏è {metric_name}")
                        elif metric_key == 'quality_score':
                            print(f"   üèÜ {metric_name}")
                        elif 'rate' in metric_key or 'success' in metric_key:
                            print(f"   üìà {metric_name}: {value:.1f}%")
                        else:
                            print(f"   ‚è±Ô∏è {metric_name}: {value:.2f}s")
                
                # Informa√ß√µes sobre compara√ß√£o de temas
                if theme_comparison:
                    print(f"\nüé≠ COMPARA√á√ÉO DE TEMAS:")
                    print(f"   üéØ Esperado: {theme_comparison['expected_theme']}")
                    print(f"   ü§ñ Detectado: {theme_comparison['detected_theme']}")
                    print(f"   ‚úÖ Status: {theme_comparison['match_status']}")
                    print(f"   üìä Score: {theme_comparison['theme_match_score']:.1f}%")
                    print(f"   üéØ Precis√£o: {theme_comparison['accuracy_score']:.1f}%")
                
                print(f"\nÔøΩ PAR√ÇMETROS REGISTRADOS:")
                param_keys = ['expected_theme', 'main_theme', 'pdf_file', 'analysis_timestamp']
                for param_key in param_keys:
                    param_col = f'params.{param_key}'
                    if param_col in latest_run and pd.notna(latest_run[param_col]):
                        value = latest_run[param_col]
                        print(f"   üìù {param_key.replace('_', ' ').title()}: {value}")
            
            print(f"\nüí° Para an√°lise detalhada:")
            print(f"   üåê Execute: mlflow ui")
            print(f"   üìä Acesse: http://localhost:5000")
            print(f"   üîç Compare experimentos e m√©tricas")
            print(f"   üìà Visualize tend√™ncias de performance")
        
    except Exception as e:
        print(f"‚ùå Erro ao obter resumo MLflow: {e}")
        print("üí° Certifique-se de que o MLflow est√° configurado corretamente")
        
        # Fallback para m√©tricas b√°sicas
        if theme_comparison:
            print(f"\nüìã Resumo b√°sico (sem MLflow):")
            print(f"   üéØ Tema esperado: {theme_comparison['expected_theme']}")
            print(f"   ü§ñ Tema detectado: {theme_comparison['detected_theme']}")
            print(f"   üìä Correspond√™ncia: {theme_comparison['match_status']}")
            print(f"   ‚è±Ô∏è Tempo total: {total_processing_time:.2f}s")
            print(f"   üèÜ Score de qualidade: {quality_score:.1f}")

if __name__ == "__main__":
    success = main()
    if success:
        print("\n‚ú® Monitoramento de PDF testado com sucesso!")
    else:
        print("\nüí• Falha no teste de monitoramento!")
