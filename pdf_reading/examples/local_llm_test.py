"""
Exemplo de uso com LLMs locais gratuitos
Demonstra como usar Ollama e Hugging Face Transformers
"""
from pathlib import Path
import sys
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Adicionar o diret√≥rio do projeto ao path
sys.path.append(str(Path(__file__).parent.parent))

def test_local_llm():
    """Testa LLMs locais dispon√≠veis"""
    print("ü§ñ Testando LLMs Locais Gratuitos")
    print("=" * 50)
    
    try:
        from llm_pdf_reading.local_llm import create_local_llm_manager
        
        # Criar manager
        manager = create_local_llm_manager()
        
        # Verificar disponibilidade
        if not manager.is_available():
            print("‚ùå Nenhum LLM local dispon√≠vel!")
            print("\nüí° Para usar LLMs gratuitos:")
            print("   1. Instale Ollama: https://ollama.ai/download")
            print("   2. Execute: ollama pull llama2:7b")
            print("   3. Ou instale transformers: pip install transformers torch")
            return False
        
        # Mostrar informa√ß√µes
        info = manager.get_info()
        print(f"‚úÖ LLM Local Dispon√≠vel!")
        print(f"   Provedor: {info['provider']}")
        if 'models' in info:
            print(f"   Modelos: {info['models']}")
        if 'model' in info:
            print(f"   Modelo: {info['model']}")
        if 'device' in info:
            print(f"   Device: {info['device']}")
        
        # Teste de gera√ß√£o
        print(f"\nüß™ Testando gera√ß√£o de texto...")
        test_prompts = [
            "Explique brevemente o que √© intelig√™ncia artificial:",
            "Resuma em uma frase: A leitura de PDFs √© importante porque",
            "Complete: Machine learning √©"
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            try:
                print(f"\n{i}. Prompt: {prompt}")
                response = manager.generate(prompt, max_length=100, temperature=0.7)
                print(f"   Resposta: {response[:200]}...")
            except Exception as e:
                print(f"   ‚ùå Erro: {e}")
        
        return True
        
    except ImportError as e:
        print(f"‚ùå Depend√™ncias n√£o instaladas: {e}")
        print("üí° Execute: pip install transformers torch ollama")
        return False
    except Exception as e:
        print(f"‚ùå Erro inesperado: {e}")
        return False

def test_pdf_with_local_llm():
    """Testa processamento de PDF com LLM local"""
    print("\nüìÑ Testando PDF com LLM Local")
    print("=" * 50)
    
    try:
        from llm_pdf_reading.orchestrator import PDFReadingOrchestrator
        
        # Criar orquestrador com modelos locais
        orchestrator = PDFReadingOrchestrator(use_local_models=True)
        
        # Verificar se LLM est√° dispon√≠vel
        llm_info = orchestrator.get_llm_info()
        print(f"ü§ñ LLM Info: {llm_info}")
        
        # Caminho para PDF de exemplo
        pdf_path = "data/raw/exemplo.pdf"
        
        if not Path(pdf_path).exists():
            print(f"‚ùå Arquivo n√£o encontrado: {pdf_path}")
            print("üí° Coloque um arquivo PDF em 'data/raw/exemplo.pdf' para testar")
            
            # Criar texto de exemplo para demonstra√ß√£o
            exemplo_texto = """
            Este √© um documento de exemplo sobre intelig√™ncia artificial.
            A IA est√° revolucionando v√°rias √°reas da tecnologia e sociedade.
            Machine learning e deep learning s√£o subcampos importantes da IA.
            Aplica√ß√µes incluem reconhecimento de voz, vis√£o computacional e processamento de linguagem natural.
            O futuro da IA promete ainda mais avan√ßos e integra√ß√£o na vida cotidiana.
            """
            
            print(f"\nüß™ Usando texto de exemplo para demonstra√ß√£o...")
            
            # Testar an√°lise de conte√∫do
            analysis = orchestrator._analyze_content_with_llm(exemplo_texto)
            print(f"\nüìä An√°lise do conte√∫do:")
            for key, value in analysis.items():
                print(f"   {key}: {value}")
            
            # Testar resposta a pergunta
            pergunta = "Quais s√£o os subcampos da intelig√™ncia artificial?"
            resposta = orchestrator.answer_question(exemplo_texto, pergunta)
            print(f"\n‚ùì Pergunta: {pergunta}")
            print(f"üí° Resposta: {resposta}")
            
            return True
        
        # Processar PDF real
        print(f"üìÑ Processando: {pdf_path}")
        result = orchestrator.process_pdf(pdf_path)
        
        if result["success"]:
            print("‚úÖ PDF processado com sucesso!")
            print(f"üìä Estat√≠sticas:")
            analysis = result["analysis"]
            print(f"   - Palavras: {analysis['word_count']}")
            print(f"   - Tipo de an√°lise: {analysis['analysis_type']}")
            print(f"   - LLM usado: {result.get('llm_used', 'N/A')}")
            
            if 'llm_summary' in analysis:
                print(f"   - Resumo LLM: {analysis['llm_summary'][:200]}...")
            
            # Testar pergunta
            pergunta = "Qual √© o tema principal do documento?"
            resposta = orchestrator.answer_question(result["content"], pergunta)
            print(f"\n‚ùì Pergunta: {pergunta}")
            print(f"üí° Resposta: {resposta[:300]}...")
        else:
            print(f"‚ùå Erro: {result['error']}")
            
    except Exception as e:
        print(f"‚ùå Erro inesperado: {e}")
        return False

def show_setup_guide():
    """Mostra guia de configura√ß√£o para LLMs locais"""
    print("\nüìö Guia de Configura√ß√£o - LLMs Locais Gratuitos")
    print("=" * 60)
    
    print("ü¶ô OP√á√ÉO 1: Ollama (Recomendado)")
    print("   1. Baixe: https://ollama.ai/download")
    print("   2. Instale o execut√°vel")
    print("   3. Abra terminal e execute:")
    print("      ollama pull llama2:7b")
    print("      ollama pull mistral:7b")
    print("   4. Teste: ollama run llama2")
    
    print("\nü§ó OP√á√ÉO 2: Hugging Face Transformers")
    print("   1. Instale depend√™ncias:")
    print("      pip install torch transformers accelerate")
    print("   2. Para GPU (NVIDIA):")
    print("      pip install torch --index-url https://download.pytorch.org/whl/cu121")
    print("   3. Configure no .env:")
    print("      HF_MODEL_NAME=microsoft/DialoGPT-medium")
    
    print("\n‚ö° DICAS DE PERFORMANCE:")
    print("   - GPU NVIDIA: Use modelos 7B (llama2:7b, mistral:7b)")
    print("   - CPU: Use modelos menores (DialoGPT-medium)")
    print("   - RAM: Recomendado 8GB+ para modelos 7B")
    print("   - Disco: Reserve 5-10GB para modelos")
    
    print("\nüîß CONFIGURA√á√ÉO NO .env:")
    print("   USE_LOCAL_MODELS=True")
    print("   OLLAMA_BASE_URL=http://localhost:11434")
    print("   OLLAMA_DEFAULT_MODEL=llama2:7b")
    print("   USE_GPU=True")

def main():
    """Fun√ß√£o principal"""
    print("üöÄ LLM PDF Reading - Teste de Modelos Locais")
    print("üéØ 100% Gratuito usando GPU local!")
    print("=" * 60)
    
    # Testar LLMs locais
    llm_available = test_local_llm()
    
    if llm_available:
        # Testar com PDF
        test_pdf_with_local_llm()
    else:
        # Mostrar guia de configura√ß√£o
        show_setup_guide()
    
    print("\n" + "=" * 60)
    print("üéâ Teste conclu√≠do!")
    print("üí° Para usar na interface web: streamlit run apps/streamlit_app.py")

if __name__ == "__main__":
    main()
