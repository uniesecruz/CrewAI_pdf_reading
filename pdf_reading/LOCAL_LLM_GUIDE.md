# ü§ñ Guia Completo - LLMs Gratuitas na GPU

## üéØ Objetivo

Este guia configura o sistema para usar **LLMs 100% gratuitas** rodando na sua GPU local, eliminando custos de APIs comerciais.

## üöÄ Setup R√°pido (Windows)

```bash
# M√©todo 1: Script autom√°tico
scripts/setup_local_llm.bat

# M√©todo 2: Manual
python scripts/setup_environment.py
```

## ü¶ô Op√ß√£o 1: Ollama (Recomendado)

### Instala√ß√£o:
1. **Baixe:** https://ollama.ai/download/windows
2. **Execute** o instalador
3. **Reinicie** o terminal

### Modelos Recomendados:
```bash
# Modelo geral (4GB RAM)
ollama pull llama2:7b

# R√°pido e eficiente (4GB RAM)  
ollama pull mistral:7b

# Especializado em c√≥digo (4GB RAM)
ollama pull codellama:7b

# Conversacional (4GB RAM)
ollama pull neural-chat:7b
```

### Teste:
```bash
ollama run llama2
>>> Ol√°! Como voc√™ est√°?
```

## ü§ó Op√ß√£o 2: Hugging Face Transformers

### Instala√ß√£o:
```bash
# CPU
pip install torch transformers accelerate

# GPU NVIDIA (recomendado)
pip install torch --index-url https://download.pytorch.org/whl/cu121
pip install transformers accelerate bitsandbytes
```

### Modelos Recomendados:
- **microsoft/DialoGPT-medium** - Conversacional (400MB)
- **microsoft/DialoGPT-large** - Melhor qualidade (1.3GB)
- **distilbert-base-uncased** - Embeddings r√°pidos (250MB)

## ‚öôÔ∏è Configura√ß√£o (.env)

```env
# ========================================
# MODELOS LOCAIS GRATUITOS (Recomendado)
# ========================================

USE_LOCAL_MODELS=True

# Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=llama2:7b

# Hugging Face
HF_MODEL_NAME=microsoft/DialoGPT-medium
HF_CACHE_DIR=./models/huggingface

# GPU
USE_GPU=True
DEVICE=auto  # auto, cpu, cuda

# Performance
MAX_TOKENS=2048
TEMPERATURE=0.7
```

## üéÆ Requisitos de Hardware

### GPU NVIDIA (Recomendado):
- **VRAM:** 6GB+ para modelos 7B
- **RAM:** 8GB+ sistema
- **Disco:** 10GB+ livre

### CPU (Alternativa):
- **RAM:** 16GB+ recomendado
- **CPU:** 4+ cores
- **Disco:** 5GB+ livre

## üß™ Teste do Sistema

```bash
# Teste completo
python examples/local_llm_test.py

# Teste b√°sico
python examples/basic_usage.py

# Interface web
streamlit run apps/streamlit_app.py
```

## üìä Compara√ß√£o de Modelos

| Modelo | Tamanho | VRAM | Qualidade | Velocidade |
|--------|---------|------|-----------|------------|
| llama2:7b | 4GB | 6GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| mistral:7b | 4GB | 6GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| DialoGPT-medium | 400MB | 2GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| codellama:7b | 4GB | 6GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |

## üîß Solu√ß√£o de Problemas

### Erro: "CUDA out of memory"
```bash
# Usar modelo menor
ollama pull mistral:7b

# Ou configurar CPU
export DEVICE=cpu
```

### Erro: "Ollama n√£o encontrado"
```bash
# Verificar instala√ß√£o
ollama --version

# Verificar servi√ßo
ollama serve
```

### Erro: "Import torch could not be resolved"
```bash
# Reinstalar PyTorch
pip uninstall torch
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

## üí° Dicas de Performance

### Para GPU:
- Use modelos 7B para melhor qualidade
- Configure `device=cuda` no .env
- Monitore VRAM com `nvidia-smi`

### Para CPU:
- Use modelos menores (DialoGPT-medium)
- Configure `device=cpu` no .env
- Aumente RAM se poss√≠vel

### Para ambos:
- Feche outros programas pesados
- Use SSD para cache de modelos
- Configure `temperature=0.3` para respostas mais precisas

## üåü Vantagens dos Modelos Locais

‚úÖ **100% Gratuito** - Sem custos de API  
‚úÖ **Privacidade** - Dados ficam no seu computador  
‚úÖ **Sem limite de tokens** - Use quanto quiser  
‚úÖ **Offline** - Funciona sem internet  
‚úÖ **Customiz√°vel** - Ajuste par√¢metros livremente  
‚úÖ **Sem rate limits** - Sem limita√ß√µes de uso  

## üöÄ Pr√≥ximos Passos

1. **Configure** usando o script autom√°tico
2. **Teste** com `python examples/local_llm_test.py`
3. **Use** a interface web com `streamlit run apps/streamlit_app.py`
4. **Experimente** diferentes modelos e par√¢metros
5. **Customize** para suas necessidades espec√≠ficas

---

**üéâ Parab√©ns!** Agora voc√™ tem um sistema completo de LLMs gratuitas rodando na sua GPU!
