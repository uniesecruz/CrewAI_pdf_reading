#!/usr/bin/env python3
"""
Teste para verificar se as corre√ß√µes do LLM funcionam
"""
import sys
from pathlib import Path

# Adicionar o m√≥dulo ao path
sys.path.append(str(Path(__file__).parent / "pdf_reading"))

def test_local_llm():
    """Testa o gerenciador de LLM local"""
    try:
        from llm_pdf_reading.local_llm import LocalLLMManager
        
        # Configura√ß√£o de teste
        config = {
            'use_ollama': True,
            'use_huggingface': True,
            'ollama_url': 'http://localhost:11434',
            'ollama_model': 'llama2:7b',
            'hf_model': 'microsoft/DialoGPT-medium',
            'device': 'auto'
        }
        
        print("üîß Testando LocalLLMManager...")
        manager = LocalLLMManager(config)
        
        print(f"‚úÖ Manager criado: {manager.current_provider}")
        print(f"üìä Dispon√≠vel: {manager.is_available()}")
        
        if manager.is_available():
            print("üß™ Testando gera√ß√£o com max_length...")
            try:
                response = manager.generate(
                    "Teste simples",
                    max_length=50,
                    temperature=0.5
                )
                print(f"‚úÖ Sucesso: {response[:100]}...")
            except Exception as e:
                print(f"‚ùå Erro na gera√ß√£o: {e}")
        else:
            print("‚ö†Ô∏è Nenhum LLM dispon√≠vel para teste")
            
    except Exception as e:
        print(f"‚ùå Erro no teste: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_local_llm()
